{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we have a sentence which we want to do some calculation on for a ML model. How migh we go about doing that?\n",
    "\n",
    "One approach is\n",
    "\n",
    "1. Divide the sentence into it's individual words. A process called tokenization.\n",
    "2. Label each word with a number which we can use for computation. A process called numericalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The': 0,\n",
       " 'brown': 1,\n",
       " 'dog': 2,\n",
       " 'fox': 3,\n",
       " 'jumps': 4,\n",
       " 'lazy': 5,\n",
       " 'over': 6,\n",
       " 'quick': 7,\n",
       " 'the': 8}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'The quick brown fox jumps over the lazy dog'\n",
    "tokens = sentence.split()\n",
    "vocabulary = sorted(set(tokens))\n",
    "\n",
    "word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "index_to_word = {i: word for i, word in enumerate(vocabulary)}\n",
    "\n",
    "word_to_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can supercharge this approach by generating a corresponding vector to each token of some size n where we can capture the meaning of the words. The vector would be initialized with random value, capturing no meaning. However, this vector can then be incorporated in the training process, allowing us to learn it.\n",
    "\n",
    "In other words, the same way we calculate how the weights and biases of a network need to change in order to get the right output, we can calculate how the vector needs to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.383624  ,  0.29444485, -1.03983432, -0.47507847])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_embeddings(vocabulary_size, embedding_size):\n",
    "    return np.random.randn(vocabulary_size, embedding_size)\n",
    "\n",
    "VECTOR_SIZE = 4\n",
    "\n",
    "vocabulary_embeddings = generate_embeddings(len(vocabulary), VECTOR_SIZE)\n",
    "\n",
    "word_to_embedding = {word: vocabulary_embeddings[word_to_index[word]] for word in vocabulary}\n",
    "\n",
    "word_to_embedding['quick']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key insight\n",
    "* We can represent a token, such as a word as a vector.\n",
    "* The goal is then to find the values for the vector which best represents the token.\n",
    "* This can be understood as a way of capturing the meaning of the token.\n",
    "* In practice the meaning can be learned through the training of a model.\n",
    "* That is to say that we can include the vector as a parameter in a model, and adjust it with backpropagation.\n",
    "\n",
    "In a well trained embedding space optimized for word similarity, we would expect the following to be true:\n",
    "word_to_embedding['king'] - word_to_embedding['man'] + word_to_embedding['woman'] = word_to_embedding['queen']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
