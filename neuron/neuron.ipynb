{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the mesmerizing realm of enchanting creatures! In this world, two delightful species have captured the hearts and imaginations of many: the charming Fluffies and the intriguing Spikies. These captivating beings coexist harmoniously, yet their distinctive traits make them truly unique. Today, we embark on a thrilling quest to unveil the secrets of identifying these magical creatures based on their height and mesmerizing fur colors.\n",
    "\n",
    "The Fluffies are petite, endearing beings that exude warmth and affection. Their fur, as their name implies, is irresistibly soft and fluffy, making them the ideal cuddle buddies. They don a striking fur palette of enthralling reds and alluring blues. These affectionate creatures are always enthusiastic about making new friends and spreading cheer wherever they go.\n",
    "\n",
    "Conversely, the Spikies are taller and more audacious, adorned with spiky fur that comes in vivacious greens and resplendent yellows. Despite their seemingly fierce appearance, they possess a friendly nature and are always eager to explore their vibrant surroundings.\n",
    "\n",
    "To determine whether a magical creature belongs to the lovable Fluffies or the adventurous Spikies, we will construct a simple neuron model. By supplying the creature's height and a numerical value representing its fur color, our model will discern the species to which the creature belongs. The fur colors are encoded as follows:\n",
    "\n",
    "1 = Red\n",
    "2 = Blue\n",
    "3 = Green\n",
    "4 = Yellow\n",
    "\n",
    "The number 1 will signify the Spikies, while the number 2 will denote the Fluffies. Let us embark on this fascinating journey and unravel the mysteries of these enchanting species!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>height</th>\n",
       "      <th>color</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   height  color  species\n",
       "0     4.2      1        0\n",
       "1     4.6      2        0\n",
       "2     3.9      1        0\n",
       "3     3.8      2        0\n",
       "4     4.1      1        0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/creatures.csv')\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neuron Class\n",
    "The Neuron class represents a single neuron in a neural network. This neuron takes two inputs, processes them, and produces one output. The processing of inputs in a neuron is done using the weighted sum of inputs followed by an activation function. In this case, we're using the sigmoid activation function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self):\n",
    "        self.w = np.array([1, 1])\n",
    "        self.b = np.random.randn(1)\n",
    "\n",
    "    def sigmoid_activation(self, n):\n",
    "        return 1 / (1 + np.exp(-n))\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        return self.sigmoid_activation(np.dot(self.w, x) + self.b)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid_activation method implements the sigmoid activation function, which takes an input n and returns a value between 0 and 1. This function is particularly useful for binary classification problems, as it squashes the input value into a probability-like output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward_pass method is where the magic happens. It takes an input array x with two elements (corresponding to the two weights we initialized earlier) and computes the weighted sum of the inputs by taking the dot product of the weights and input array. Then, it adds the bias to the result. Finally, the method applies the sigmoid activation function to the weighted sum to produce the neuron's output.\n",
    "\n",
    "In summary, this simple Neuron class demonstrates the fundamental principles of how a single neuron processes its inputs and produces an output in a neural network. The weighted sum of inputs, the addition of bias, and the application of an activation function are the key components of this process. The example provided here uses the sigmoid activation function, which is particularly suitable for binary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99840438])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[0, 0:2]\n",
    "model = Neuron() \n",
    "\n",
    "# We have randomly initiated weights and biases, so the output of the forward pass will be random, here it predicts 1, meaning spiky\n",
    "model.forward_pass(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we still need to develop the neuron by adding a loss function.\n",
    "\n",
    "The loss function serves as a metric to measure the performance of the model during training. It quantifies the difference between the model's predictions and the true labels of the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self):\n",
    "        self.w = np.array([1, 1])\n",
    "        self.b = np.random.randn(1)\n",
    "\n",
    "    def sigmoid_activation(self, n):\n",
    "        return 1 / (1 + np.exp(-n))\n",
    "    \n",
    "    def binary_cross_entropy_loss(self, prediction, y):\n",
    "        return -y * np.log(prediction) - (1 - y) * np.log(1 - prediction)\n",
    "\n",
    "    def forward_pass(self, X, y=None):\n",
    "        prediction = self.sigmoid_activation(np.dot(self.w, X) + self.b)\n",
    "        loss = self.binary_cross_entropy_loss(prediction, y) if y is not None else None\n",
    "        return prediction, loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binary_cross_entropy_loss method calculates the binary cross-entropy loss between the predicted value (prediction) and the true label (y). Binary cross-entropy loss is a widely used loss function for binary classification problems, as it quantifies the difference between two probability distributions (the true label and the predicted probability). The smaller the loss, the better the prediction aligns with the true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.99951403]), array([7.62936479]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = df.iloc[0, 0:2], df.iloc[0, 2]\n",
    "\n",
    "model = Neuron() \n",
    "model.forward_pass(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this specific example, the output of the forward pass is (0.99, 7.62). The first value is the prediction, which is very close to 1, indicating a high probability for the Spiky species. The second value is the loss, which is a big positive number. Since the neuron is not yet trained, these values are generated based on the randomly initialized weights and bias, and they may not be accurate predictions.\n",
    "\n",
    "To improve the performance of the neuron, you would need to train it using a training dataset and a learning algorithm like gradient descent. The training process will adjust the weights and bias to minimize the loss function, thereby improving the model's predictions on the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a gradient descent algorithm to optimize the neuron model by backpropagating through the network to update the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self):\n",
    "        self.w = np.random.randn(2) * 0.01\n",
    "        self.b = np.random.randn() * 0.01\n",
    "\n",
    "    def sigmoid_activation(self, n):\n",
    "        return 1 / (1 + np.exp(-n))\n",
    "    \n",
    "    def sigmoid_derivative(self, n):\n",
    "        return  n * (1 - n)\n",
    "    \n",
    "    def binary_cross_entropy_loss(self, prediction, y):\n",
    "        return -y * np.log(prediction) - (1 - y) * np.log(1 - prediction)\n",
    "\n",
    "    def binary_cross_entropy_loss_derivative(self, prediction, y):\n",
    "        return -y / prediction + (1 - y) / (1 - prediction)\n",
    "\n",
    "    def backpropagation(self, X, y, prediction, learning_rate):\n",
    "        bias_gradient = self.sigmoid_derivative(prediction) * self.binary_cross_entropy_loss_derivative(prediction, y)\n",
    "        weight_gradients = [x * bias_gradient for x in X]\n",
    "        for i, gradient in enumerate(weight_gradients):\n",
    "            self.w[i] -= learning_rate * gradient\n",
    "        self.b -= learning_rate * bias_gradient\n",
    "\n",
    "    def train (self, X, y, epochs=10, learning_rate=0.05):\n",
    "        for epoch in range(epochs + 1):\n",
    "            losses = np.array([])\n",
    "            for Xi, yi in zip(X, y):\n",
    "                prediction, loss = self.forward_pass(Xi, yi)\n",
    "                losses = np.append(losses, loss)\n",
    "                self.backpropagation(Xi, yi, prediction, learning_rate)\n",
    "            if epoch < 10 or epoch % 10 == 0:\n",
    "                print(f\"Epoch: {epoch}, Loss: {losses.mean()}\")\n",
    "\n",
    "    def forward_pass(self, X, y=None):\n",
    "        prediction = self.sigmoid_activation(np.dot(self.w, X) + self.b)\n",
    "        loss = self.binary_cross_entropy_loss(prediction, y) if y is not None else None\n",
    "        return prediction, loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've extended the neuron to include a backpropagation and a training method. This allows the neuron to learn from the data by adjusting its weights and biases based on the gradient descent algorithm. Let's go through the new additions to the class.\n",
    "\n",
    "The train method is responsible for training the neuron model using the given input data X and the corresponding labels y. The method takes the number of epochs and the learning rate as optional arguments. During each epoch, the method iterates through the dataset and performs a forward pass followed by backpropagation to update the weights and biases.\n",
    "\n",
    "The train method calls the backpropagation method which computes the gradients of the loss function with respect to the weights and biases using the chain rule. It then updates the weights and biases by subtracting the gradients multiplied by the learning rate. This process helps the neuron to learn the best set of parameters that minimize the loss function.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's actually train this neuron using the dataset from the fluffy_or_spikey.csv file. The input features (height and color) are stored in the variable X, and the corresponding labels (species) are stored in the variable y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.5116093581513462\n",
      "Epoch: 1, Loss: 0.47551335959145746\n",
      "Epoch: 2, Loss: 0.446181046667073\n",
      "Epoch: 3, Loss: 0.42255401234683404\n",
      "Epoch: 4, Loss: 0.40324334235820836\n",
      "Epoch: 5, Loss: 0.3871825904555171\n",
      "Epoch: 6, Loss: 0.3735642645097868\n",
      "Epoch: 7, Loss: 0.3617845266608111\n",
      "Epoch: 8, Loss: 0.35139674004522914\n",
      "Epoch: 9, Loss: 0.3420731901373511\n",
      "Epoch: 10, Loss: 0.3335744176832586\n",
      "Epoch: 20, Loss: 0.27090396456841637\n",
      "Epoch: 30, Loss: 0.22642908062982545\n",
      "Epoch: 40, Loss: 0.19191157280337634\n",
      "Epoch: 50, Loss: 0.16464832465917464\n",
      "Epoch: 60, Loss: 0.14293238783322765\n",
      "Epoch: 70, Loss: 0.12549692029852888\n",
      "Epoch: 80, Loss: 0.11136725776304247\n",
      "Epoch: 90, Loss: 0.09979239558765919\n",
      "Epoch: 100, Loss: 0.09019894008892466\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('fluffy_or_spikey.csv')\n",
    "X, y = df[['height', 'color']].to_numpy(), df['species'].to_numpy()\n",
    "\n",
    "model = Neuron() \n",
    "model.train(X, y, epochs=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During each epoch, the neuron performs forward passes and backpropagation to update its weights and biases using the gradient descent algorithm. The model prints the average loss after each epoch.\n",
    "\n",
    "From the output, you can observe that the loss is decreasing over the epochs, which indicates that the model is learning to minimize the binary cross-entropy loss and improve its classification performance. The loss converges to a relatively stable value (around 0.09) after 100 epochs, suggesting that the model has learned a reasonably good set of parameters (weights and bias) to classify the Fluffies and Spikies based on their height and color.\n",
    "\n",
    "We can take a peak on what values the parameters settled into by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3011133 2.4318791] -8.278508652264934\n"
     ]
    }
   ],
   "source": [
    "print(model.w, model.b) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it on the same dataset it was trained on. Normally the dataset will be divieded into a train and test set we dont mind here as we're mainly interested in how the neuron functions.\n",
    "\n",
    "We'll select a batch of 10 samples from the shuffled dataset and calculate the accuracy of the model's predictions. For each sample, you perform a forward pass to obtain the prediction, and then compare it to the actual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0.77122756704925, Actual: 1\n",
      "Prediction: 0.008470814812538148, Actual: 0\n",
      "Prediction: 0.09882959516903161, Actual: 0\n",
      "Prediction: 0.7604290251040118, Actual: 1\n",
      "Prediction: 0.9705902508507176, Actual: 1\n",
      "Prediction: 0.11612696761861223, Actual: 0\n",
      "Prediction: 0.008991895691369196, Actual: 0\n",
      "Prediction: 0.009833628112717862, Actual: 0\n",
      "Prediction: 0.1071715847386691, Actual: 0\n",
      "Prediction: 0.10432425201528192, Actual: 0\n",
      "---\n",
      "Accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "X_test, y_test = df[['height', 'color']].to_numpy(), df['species'].to_numpy()\n",
    "\n",
    "test_batch = 10\n",
    "correct = 0\n",
    "\n",
    "for Xi, yi in zip(X_test[:test_batch], y_test[:test_batch]):\n",
    "    prediction, loss = model.forward_pass(Xi, yi)\n",
    "    correct += 1 if (prediction > 0.5 and yi == 1) or (prediction <= 0.5 and yi == 0) else 0\n",
    "    print(f\"Prediction: {prediction}, Actual: {yi}\")\n",
    "    \n",
    "print(\"---\")\n",
    "print(f\"Accuracy: {correct / test_batch * 100}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output, you can see the model's predictions for each sample, along with the actual label. In this case, the model achieved a stunning accuracy of 100%. Meaning a single neuron manages to fit the relationship perfectly. This is called overfitting, but thats a story for another time.\n",
    "\n",
    "The results achieved with just one neuron in the model are remarkable. It's fascinating to see that a single neuron can manage to provide perfectly accurate estimates for this dataset, which is a testament to the power of simple neural models. The neuron essentially learns a relationship between the features (height and color) and the target variable (species), allowing it to classify the creatures.\n",
    "\n",
    "However, there are a few caveats to consider. In this example, we have a relatively simple dataset with only two features, and the relationship between these features and the target variable might be straightforward enough for a single neuron to capture. In real-world scenarios, datasets often have many more features and exhibit complex relationships that a single neuron would struggle to represent.\n",
    "\n",
    "In such cases, multiple neurons are required to capture these complex relationships, and they are usually organized in layers to form a neural network. By chaining neurons together in a network, we can create models that are capable of learning hierarchical representations, with each layer learning increasingly abstract features. This enables neural networks to tackle more challenging problems and datasets.\n",
    "\n",
    "Additionally, in real-world scenarios, when working with datasets, it's crucial to split the data into a training set and a test set (sometimes also a validation set). This is done to evaluate the performance of the model and ensure that it can generalize well to new, unseen data. \n",
    "\n",
    "However, there is an undeniable charm surrounding the solitary neuron that prevails, and as its story continues to unfold, this concept may eventually be remembered as the pinnacle of human achievement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
